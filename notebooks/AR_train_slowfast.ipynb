{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1996,
     "status": "ok",
     "timestamp": 1745362581835,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "_QvzluuYcJxd",
    "outputId": "066333a5-65b4-420d-a249-293a1cb97ae4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2211,
     "status": "ok",
     "timestamp": 1745341530721,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "6ZCMDY7pcYrN",
    "outputId": "c2578f50-a844-46bf-8195-78d99fc4d3bb"
   },
   "outputs": [],
   "source": [
    "pip install pytorchvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1745341532539,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "phg5A-H_ceay"
   },
   "outputs": [],
   "source": [
    "videos_path = \"drive/MyDrive/clips/\"\n",
    "annotations_path = \"drive/MyDrive/clips/action_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1745341533761,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "gDoSTrfzDQv6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1745341534004,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "1B09KmyTDPsa",
    "outputId": "14c1ceec-1e6f-4824-90b5-41f81ed45ac5"
   },
   "outputs": [],
   "source": [
    "#load csv\n",
    "df = pd.read_csv(annotations_path)\n",
    "print(df.head())\n",
    "\n",
    "df[\"filename\"] = df[\"filename\"].apply(lambda x: os.path.join(videos_path, x))\n",
    "fixed_csv_path = \"/content/fixed_actions_labels_absolute.csv\"\n",
    "df.to_csv(fixed_csv_path, index=False)\n",
    "\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "train_csv = \"/content/train_annotations.csv\"\n",
    "test_csv = \"/content/test_annotations.csv\"\n",
    "train_df.to_csv(train_csv)\n",
    "test_df.to_csv(test_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 665
    },
    "executionInfo": {
     "elapsed": 1280,
     "status": "ok",
     "timestamp": 1744724690011,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "VZLQi08JD1lj",
    "outputId": "2e2ea67e-c345-457f-e9c9-8f07f48b2a2f"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "train_label_counts = train_df[\"action_label\"].value_counts()\n",
    "test_label_counts = test_df[\"action_label\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=train_label_counts.index, y=train_label_counts.values, color=\"blue\", alpha=0.7, label=\"Train\")\n",
    "sns.barplot(x=test_label_counts.index, y=test_label_counts.values, color=\"red\", alpha=0.7, label=\"Test\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Label Distribution in Train and Test Splits\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lIfq-4ejS4E"
   },
   "source": [
    "I tested balancing the dataset by limiting the population in each set, results weren't as good on the validation set and overfitting happened instantly, mainly due to a lot less data - so I just kept the train/test split 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70162,
     "status": "ok",
     "timestamp": 1745337996845,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "aepiTgOIGDD8",
    "outputId": "46ebace3-a175-4b74-dcff-c50adbe18dba"
   },
   "outputs": [],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swtTkBwLrYst"
   },
   "source": [
    "## NOTE ABOUT PYTORCH VIDEO TRANSFORMS: augmentations.py needs to be changed from funcional_tensor to functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3039,
     "status": "ok",
     "timestamp": 1745341541249,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "dJQKR-1xFsdy"
   },
   "outputs": [],
   "source": [
    "#This code was obtained from https://pytorch.org/hub/facebookresearch_pytorchvideo_slowfast/ , specifically the packpathway\n",
    "import torch\n",
    "from pytorchvideo.transforms import ApplyTransformToKey, UniformTemporalSubsample, RandomShortSideScale, \\\n",
    "    ShortSideScale, Normalize\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, Lambda, RandomCrop, RandomHorizontalFlip, CenterCrop\n",
    "\n",
    "side_size = 256\n",
    "max_size = 320\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "clip_duration = (num_frames * sampling_rate) / frames_per_second\n",
    "\n",
    "\n",
    "class PackPathway(nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=4):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, frames):\n",
    "        fast_pathway = frames\n",
    "        # perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(frames, 1,\n",
    "                                          torch.linspace(0, frames.shape[1] - 1, frames.shape[1] // self.alpha).long())\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "\n",
    "train_transform = ApplyTransformToKey(key=\"video\", transform=Compose(\n",
    "    [UniformTemporalSubsample(num_frames), Lambda(lambda x: x / 255.0), Normalize(mean, std),\n",
    "     RandomShortSideScale(min_size=side_size, max_size=max_size), RandomCrop(crop_size), RandomHorizontalFlip(),\n",
    "     PackPathway()]))\n",
    "test_transform = ApplyTransformToKey(key=\"video\", transform=Compose(\n",
    "    [UniformTemporalSubsample(num_frames), Lambda(lambda x: x / 255.0), Normalize(mean, std),\n",
    "     ShortSideScale(size=side_size), CenterCrop(crop_size), PackPathway()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yjy2PKpZpKPU"
   },
   "source": [
    "## Train Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1745341542867,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "1bpl3KM6pMiN"
   },
   "outputs": [],
   "source": [
    "csv_path = \"/content/train_annotations.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df[\"filename\"] = df[\"filename\"].apply(lambda x: os.path.join(\"/content\", x.replace(\"\\\\\", \"/\")).replace(\"\\\\\", \"/\"))\n",
    "\n",
    "fixed_csv_path = \"/content/fixed_actions_labels_absolute_train1.csv\"\n",
    "df.to_csv(fixed_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1745341544039,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "Vg97Yo3Hpcd6"
   },
   "outputs": [],
   "source": [
    "csv_path_test = '/content/test_annotations.csv'\n",
    "\n",
    "df = pd.read_csv(csv_path_test)\n",
    "\n",
    "df[\"filename\"] = df[\"filename\"].apply(lambda x: os.path.join(\"/content\", x.replace(\"\\\\\", \"/\")).replace(\"\\\\\", \"/\"))\n",
    "\n",
    "fixed_csv_path_test = \"/content/fixed_actions_labels_absolute_test.csv\"\n",
    "df.to_csv(fixed_csv_path_test, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1745341545122,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "_XNHJZxQptxS",
    "outputId": "68afe010-2860-4e52-e4c3-d7c314c9719d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(fixed_csv_path)\n",
    "\n",
    "#ensure correct path for training\n",
    "\n",
    "labeled_video_paths_train = [(row['filename'].replace('\\\\', '/'), {\"action_number\": row['action_number']}) for _, row in df.iterrows()]\n",
    "\n",
    "print(labeled_video_paths_train[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1745341546253,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "3j3KoHpFp2ir",
    "outputId": "60f96b51-b54c-46fc-b0a0-7949e863cee9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(fixed_csv_path_test)\n",
    "\n",
    "#ensure correct path for testing\n",
    "labeled_video_paths_test = [(row['filename'].replace('\\\\', '/'), {\"action_number\": row['action_number']}) for _, row in df.iterrows()]\n",
    "\n",
    "print(labeled_video_paths_test[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuKq8_wRqguJ"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1745341548059,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "cibSgHX_q1Ig",
    "outputId": "4e8c5875-9d58-4394-f559-16384708fb31"
   },
   "outputs": [],
   "source": [
    "print(f\"Total videos in dataset: {len(labeled_video_paths_train)}\")\n",
    "print(f\"Total videos in dataset: {len(labeled_video_paths_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1745341549589,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "j067G8x6J20R"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pytorchvideo.data import make_clip_sampler, LabeledVideoDataset\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "#dataset load\n",
    "train_data = LabeledVideoDataset(\n",
    "  labeled_video_paths=labeled_video_paths_train,\n",
    "  clip_sampler=make_clip_sampler(\"random\", 2.0),\n",
    "  transform=train_transform,\n",
    "  decode_audio=False\n",
    ")\n",
    "test_data = LabeledVideoDataset(\n",
    "  labeled_video_paths=labeled_video_paths_test,\n",
    "  clip_sampler=make_clip_sampler(\"random\", 2.0),\n",
    "  transform=test_transform,\n",
    "  decode_audio=False\n",
    ")\n",
    "\n",
    "#get the length of an iterable dataset\n",
    "class LabeledVideoDatasetWrapper(torch.utils.data.IterableDataset):\n",
    "  def __init__(self, dataset, length_estimate):\n",
    "    self.dataset = dataset\n",
    "    self.length_estimate = length_estimate\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.length_estimate\n",
    "\n",
    "  def __iter__(self):\n",
    "    return iter(self.dataset)\n",
    "\n",
    "#change based on dataset size, I used 4915 (80%) and 1229 (20%)\n",
    "dataset_length = 4915\n",
    "wrapped_train_data = LabeledVideoDatasetWrapper(train_data, dataset_length)\n",
    "dataset_test_length = 1229\n",
    "wrapped_test_data = LabeledVideoDatasetWrapper(test_data, dataset_test_length)\n",
    "\n",
    "\n",
    "#dataloading, using A100 16 batch size works, 8 for T4.\n",
    "train_loader = DataLoader(wrapped_train_data, batch_size=16, num_workers=4, persistent_workers=True)\n",
    "test_loader = DataLoader(wrapped_test_data, batch_size=16, num_workers=4, persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52I_fbE9qjS0"
   },
   "source": [
    "## Load SlowFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1548,
     "status": "ok",
     "timestamp": 1745341619000,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "Ruv6GgPlLlo0",
    "outputId": "ec475b96-e02d-42a0-dfb4-a7d8f37700e6"
   },
   "outputs": [],
   "source": [
    "#load slowfast model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"slowfast_r50\"\n",
    "#pretrained on kinetics-400 and finetuning on multisports-football\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n",
    "model = model.to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1745341655370,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "BOw1V249dsD_",
    "outputId": "6cbbfa94-de58-49e5-eb0f-b15090769b2d"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"slowfast_r50\"\n",
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/trained_model_epoch_new_bs_16_lr_0.0001_new1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1745341652509,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "7hjD8sXtkrBY",
    "outputId": "d6885376-9efb-4476-d919-5a486e75f76e"
   },
   "outputs": [],
   "source": [
    "for module in model.modules():\n",
    "  if isinstance(module, torch.nn.Dropout):\n",
    "    #check dropout, can change if needed for overfitting\n",
    "    print(module.p)\n",
    "    module.p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1745341653318,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "bZLk5cwFLrxu",
    "outputId": "516f5ddb-d8ac-469b-9b87-6075157e19b8"
   },
   "outputs": [],
   "source": [
    "num_classes = len(df[\"action_number\"].unique())\n",
    "#final layer changed to 15 classes\n",
    "model.blocks[-1].proj = nn.Linear(model.blocks[-1].proj.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "print(num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aufaq5oQmzWY"
   },
   "source": [
    "Used ADAM and SGD, ADAM began overfitting ~3-4 epochs, SGD is more stable but takes more time and will usually eventually give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXPuVcGXohqB"
   },
   "outputs": [],
   "source": [
    "#lr = 0.0001, wd= 0.0001, bs = 8, epochs = 3, val acc = 0.539\n",
    "#lr = 0.0001, wd= 0.0001, bs = 16, epochs = 3, val acc = 0.545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745341668217,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "WHn7P4KXLzY8"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam,SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "#optimiser and loss\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13341943,
     "status": "ok",
     "timestamp": 1745357357264,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "-A541gxWMEaj",
    "outputId": "be508659-c6c8-4828-8ab2-08feabd28ec1"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def pad_videos(videos, device):\n",
    "  #find the max length clip, pad everything to that\n",
    "  max_length = max(v.shape[2] for v in videos)\n",
    "  padded_videos = [torch.nn.functional.pad(v, (0, 0, 0, 0, 0, max_length - v.shape[2])) for v in videos]\n",
    "  return torch.stack(padded_videos).to(device)\n",
    "\n",
    "#minimum label value\n",
    "LABEL_OFFSET = 33\n",
    "\n",
    "def train(model, dataloader, optimiser, loss_fn, device):\n",
    "  #training loop\n",
    "  model.train()\n",
    "  total_loss, total_acc = 0, 0\n",
    "\n",
    "  for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "    videos = [v.to(device) for v in batch[\"video\"]]\n",
    "    labels = batch[\"action_number\"].to(device) - LABEL_OFFSET\n",
    "    labels = labels.long()\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    preds = model(videos)\n",
    "    loss = loss_fn(preds, labels)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_acc += (preds.argmax(dim=-1) == labels).sum().item()\n",
    "\n",
    "  return total_loss / len(dataloader), total_acc / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "  #evaluation loop\n",
    "  model.eval()\n",
    "  total_loss, total_acc = 0, 0\n",
    "  with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "      videos = [v.to(device) for v in batch[\"video\"]]\n",
    "      labels = batch[\"action_number\"].to(device) - LABEL_OFFSET\n",
    "      labels = labels.long()\n",
    "\n",
    "      preds = model(videos)\n",
    "      loss = loss_fn(preds, labels)\n",
    "\n",
    "      total_loss += loss.item()\n",
    "      total_acc += (preds.argmax(dim=-1) == labels).sum().item()\n",
    "\n",
    "  return total_loss / len(dataloader), total_acc / len(dataloader.dataset)\n",
    "\n",
    "#train for x amount of epochs (30 here)\n",
    "epochs = 8\n",
    "for epoch in range(1, epochs + 1):\n",
    "  #main loop\n",
    "  loss, acc = train(model, train_loader, optimiser, loss_fn, device)\n",
    "  print(\"Epoch: \", epoch,\"Loss: \", loss,\"Accuracy: \", acc)\n",
    "\n",
    "  val_loss, val_acc = evaluate(model, test_loader, loss_fn, device)\n",
    "  print(\"Epoch: \", epoch,\"Validation Loss: \", val_loss,\"Validation Accuracy: \", val_acc)\n",
    "\n",
    "#model saved for\n",
    "  #save for each epoch\n",
    "  model_save_path = \"/content/drive/MyDrive/trained_model_epoch_new_bs_16_lr_0.0001_new\" + str(epoch) + \".pth\"\n",
    "  torch.save(model.state_dict(), model_save_path)\n",
    "  print(\"Model Saved\")\n",
    "\n",
    "print(\"Training Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4py6imOIdTEi"
   },
   "outputs": [],
   "source": [
    "# Training: 100%|██████████| 308/308 [32:35<00:00,  6.35s/it]\n",
    "\n",
    "# Epoch:  1 Loss:  1.8155140389095654 Accuracy:  0.37029501525940994\n",
    "\n",
    "# Evaluating: 80it [08:06,  6.08s/it]\n",
    "\n",
    "# Epoch:  1 Validation Loss:  1.615652519387084 Validation Accuracy:  0.4646053702196908\n",
    "\n",
    "# Training: 100%|██████████| 308/308 [25:56<00:00,  5.05s/it]\n",
    "\n",
    "# Epoch:  1 Loss:  1.5011588400834566 Accuracy:  0.4799593082400814\n",
    "\n",
    "# Evaluating: 80it [06:26,  4.83s/it]\n",
    "\n",
    "# Epoch:  1 Validation Loss:  1.3868585677890035 Validation Accuracy:  0.5207485760781123\n",
    "# Model Saved\n",
    "\n",
    "# Training: 100%|██████████| 308/308 [25:58<00:00,  5.06s/it]\n",
    "\n",
    "# Epoch:  2 Loss:  1.3306402080244832 Accuracy:  0.5340793489318413\n",
    "\n",
    "# Evaluating: 80it [06:32,  4.91s/it]\n",
    "\n",
    "# Epoch:  2 Validation Loss:  1.409806552645448 Validation Accuracy:  0.5305126118795769\n",
    "# Model Saved\n",
    "\n",
    "# Training: 100%|██████████| 308/308 [26:10<00:00,  5.10s/it]\n",
    "\n",
    "# Epoch:  3 Loss:  1.2170358844004667 Accuracy:  0.5623601220752797\n",
    "\n",
    "# Evaluating: 80it [06:31,  4.89s/it]\n",
    "\n",
    "# Epoch:  3 Validation Loss:  1.3789514612841915 Validation Accuracy:  0.5443449959316518\n",
    "# Model Saved\n",
    "\n",
    "# Training: 100%|██████████| 308/308 [26:05<00:00,  5.08s/it]\n",
    "\n",
    "# Epoch:  4 Loss:  1.1126136767206254 Accuracy:  0.5973550356052899\n",
    "\n",
    "# Evaluating: 80it [06:31,  4.90s/it]\n",
    "\n",
    "# Epoch:  4 Validation Loss:  1.3977933146736838 Validation Accuracy:  0.5305126118795769\n",
    "# Model Saved\n",
    "\n",
    "# Training: 100%|██████████| 308/308 [26:18<00:00,  5.13s/it]\n",
    "\n",
    "# Epoch:  5 Loss:  1.0285702541277006 Accuracy:  0.6313326551373347\n",
    "\n",
    "# Evaluating: 80it [06:31,  4.89s/it]\n",
    "\n",
    "# Epoch:  5 Validation Loss:  1.5538488897410305 Validation Accuracy:  0.5036615134255492\n",
    "# Model Saved\n",
    "\n",
    "# Training: 100%|██████████| 308/308 [26:08<00:00,  5.09s/it]\n",
    "\n",
    "# Epoch:  6 Loss:  0.9428888381301582 Accuracy:  0.6573753814852492\n",
    "\n",
    "# Evaluating: 80it [06:30,  4.88s/it]\n",
    "\n",
    "# Epoch:  6 Validation Loss:  1.55042182238071 Validation Accuracy:  0.5264442636289667\n",
    "# Model Saved\n",
    "\n",
    "# Training: 100%|██████████| 308/308 [26:05<00:00,  5.08s/it]\n",
    "\n",
    "# Epoch:  7 Loss:  0.8717771956285874 Accuracy:  0.6826042726347915\n",
    "\n",
    "# Evaluating: 80it [06:25,  4.82s/it]\n",
    "\n",
    "# Epoch:  7 Validation Loss:  1.5754852310403602 Validation Accuracy:  0.5191212367778681\n",
    "# Model Saved\n",
    "\n",
    "# Training: 100%|██████████| 308/308 [26:06<00:00,  5.08s/it]\n",
    "\n",
    "# Epoch:  8 Loss:  0.8049682320712449 Accuracy:  0.7133265513733469\n",
    "\n",
    "# Evaluating: 80it [06:31,  4.89s/it]\n",
    "\n",
    "# Epoch:  8 Validation Loss:  1.6961428283096909 Validation Accuracy:  0.48494711147274205\n",
    "# Model Saved\n",
    "# Training Done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7U9S4jWgWR99"
   },
   "outputs": [],
   "source": [
    "# Training: 100%|██████████| 308/308 [35:21<00:00,  6.89s/it]\n",
    "\n",
    "# Epoch:  1 Loss:  1.9765217513233035 Accuracy:  0.37741607324516785\n",
    "\n",
    "# Evaluating: 80it [07:04,  5.31s/it]\n",
    "\n",
    "# Epoch:  1 Validation Loss:  1.777498489076441 Validation Accuracy:  0.40602115541090317\n",
    "# Model Saved\n",
    "\n",
    "# Training: 100%|██████████| 308/308 [28:11<00:00,  5.49s/it]\n",
    "\n",
    "# Epoch:  2 Loss:  1.678143762535863 Accuracy:  0.4233977619532045\n",
    "\n",
    "# Evaluating: 80it [07:06,  5.33s/it]\n",
    "\n",
    "# Epoch:  2 Validation Loss:  1.7982588526490446 Validation Accuracy:  0.4157851912123678\n",
    "# Model Saved\n",
    "\n",
    "# Training: 100%|██████████| 308/308 [28:08<00:00,  5.48s/it]\n",
    "\n",
    "# Epoch:  3 Loss:  1.6139012399044903 Accuracy:  0.4516785350966429\n",
    "\n",
    "# Evaluating: 80it [07:09,  5.37s/it]\n",
    "\n",
    "# Epoch:  3 Validation Loss:  4.347201814899197 Validation Accuracy:  0.33848657445077296\n",
    "# Model Saved\n",
    "\n",
    "# Training: 100%|██████████| 308/308 [28:01<00:00,  5.46s/it]\n",
    "\n",
    "# Epoch:  4 Loss:  1.5645483021999333 Accuracy:  0.46897253306205494\n",
    "\n",
    "# Evaluating: 80it [07:07,  5.35s/it]\n",
    "\n",
    "# Epoch:  4 Validation Loss:  1.6486872976476497 Validation Accuracy:  0.4499593165174939\n",
    "# Model Saved\n",
    "# Training Done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5Q42gzCLtEA"
   },
   "outputs": [],
   "source": [
    "# Training: 100%|██████████| 308/308 [27:58<00:00,  5.45s/it]\n",
    "\n",
    "# Epoch:  1 Loss:  1.782033293858751 Accuracy:  0.392675483214649\n",
    "\n",
    "# Evaluating: 80it [06:59,  5.24s/it]\n",
    "\n",
    "# Epoch:  1 Validation Loss:  1.557073233189521 Validation Accuracy:  0.47843775427176566\n",
    "# Model Saved\n",
    "\n",
    "# Training: 100%|██████████| 308/308 [28:11<00:00,  5.49s/it]\n",
    "\n",
    "# Epoch:  2 Loss:  1.4655607015668572 Accuracy:  0.488911495422177\n",
    "\n",
    "# Evaluating: 80it [07:04,  5.30s/it]\n",
    "\n",
    "# Epoch:  2 Validation Loss:  1.3902774453163147 Validation Accuracy:  0.5451586655817738\n",
    "# Model Saved\n",
    "\n",
    "# Training: 100%|██████████| 308/308 [28:10<00:00,  5.49s/it]\n",
    "\n",
    "# Epoch:  3 Loss:  1.322447316987174 Accuracy:  0.5310274669379451\n",
    "\n",
    "# Evaluating: 80it [07:10,  5.38s/it]\n",
    "\n",
    "# Epoch:  3 Validation Loss:  1.3320982061423265 Validation Accuracy:  0.5573637103336045\n",
    "# Model Saved\n",
    "\n",
    "# Training: 100%|██████████| 308/308 [28:25<00:00,  5.54s/it]\n",
    "\n",
    "# Epoch:  4 Loss:  1.1938220848897836 Accuracy:  0.5778229908443541\n",
    "\n",
    "# Evaluating: 80it [07:04,  5.30s/it]\n",
    "\n",
    "# Epoch:  4 Validation Loss:  1.3893521422302568 Validation Accuracy:  0.5378356387306753\n",
    "# Model Saved\n",
    "# Training Done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uG48Zh2Fm-3n"
   },
   "outputs": [],
   "source": [
    "# Training: 616it [34:49,  3.39s/it]\n",
    "\n",
    "# Epoch:  1 Loss:  1.7977875458515757 Accuracy:  0.3873855544252289\n",
    "\n",
    "# Evaluating: 156it [08:32,  3.28s/it]\n",
    "\n",
    "# Epoch:  1 Validation Loss:  1.5691923293974492 Validation Accuracy:  0.4882017900732303\n",
    "# Model Saved\n",
    "\n",
    "# Training: 616it [28:26,  2.77s/it]\n",
    "\n",
    "# Epoch:  2 Loss:  1.5245900104685528 Accuracy:  0.47466937945066123\n",
    "\n",
    "# Evaluating: 156it [07:00,  2.70s/it]\n",
    "\n",
    "# Epoch:  2 Validation Loss:  1.4821216936235304 Validation Accuracy:  0.5117982099267697\n",
    "# Model Saved\n",
    "\n",
    "# Training: 616it [28:40,  2.79s/it]\n",
    "\n",
    "# Epoch:  3 Loss:  1.376072660306605 Accuracy:  0.5143438453713123\n",
    "\n",
    "# Evaluating: 156it [06:56,  2.67s/it]\n",
    "\n",
    "# Epoch:  3 Validation Loss:  1.3971749685414425 Validation Accuracy:  0.5386493083807974\n",
    "# Model Saved\n",
    "\n",
    "# Training: 616it [28:33,  2.78s/it]\n",
    "\n",
    "# Epoch:  4 Loss:  1.2782971857039909 Accuracy:  0.5481180061037639\n",
    "\n",
    "# Evaluating: 156it [06:59,  2.69s/it]\n",
    "\n",
    "# Epoch:  4 Validation Loss:  1.441618792422406 Validation Accuracy:  0.5394629780309195\n",
    "# Model Saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1745362608230,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "xuRdyM3t1QY2",
    "outputId": "976dddc6-1262-449d-d1f4-5307ed07d493"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/trained_model_epoch_new_bs_16_lr_0.0001_new5.pth\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1745362613907,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "nqMXs5HY1Nfl"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "LABEL_OFFSET = 33\n",
    "def evaluate_with_metrics(model, dataloader, loss_fn, device, num_classes):\n",
    "  #evaluation with top-k and classification metrics\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  top1_correct = 0\n",
    "  top3_correct = 0\n",
    "  top5_correct = 0\n",
    "\n",
    "  all_preds = []\n",
    "  all_labels = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"evaluating with metrics\"):\n",
    "      videos = [v.to(device) for v in batch[\"video\"]]\n",
    "      labels = batch[\"action_number\"].to(device) - LABEL_OFFSET\n",
    "      labels = labels.long()\n",
    "\n",
    "      preds = model(videos)\n",
    "      loss = loss_fn(preds, labels)\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      topk_preds = preds.topk(5, dim=1).indices\n",
    "      labels_expanded = labels.view(-1, 1)\n",
    "\n",
    "      top1_correct += (topk_preds[:, :1] == labels_expanded).sum().item()\n",
    "      top3_correct += (topk_preds[:, :3] == labels_expanded).any(dim=1).sum().item()\n",
    "      top5_correct += (topk_preds == labels_expanded).any(dim=1).sum().item()\n",
    "\n",
    "      all_preds.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "      all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "  avg_loss = total_loss / len(dataloader)\n",
    "  total_samples = len(dataloader.dataset)\n",
    "\n",
    "  top1_acc = top1_correct / total_samples\n",
    "  top3_acc = top3_correct / total_samples\n",
    "  top5_acc = top5_correct / total_samples\n",
    "\n",
    "  conf_matrix = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "  precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "  recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "  f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "  #print metrics\n",
    "  print(\"Validation loss:\", avg_loss)\n",
    "  print(\"top-1 accuracy:\", top1_acc)\n",
    "  print(\"top-3 accuracy:\", top3_acc)\n",
    "  print(\"top-5 accuracy:\", top5_acc)\n",
    "  print(\"Precision (macro):\", precision)\n",
    "  print(\"Recall (macro):\", recall)\n",
    "  print(\"F1 score (macro):\", f1)\n",
    "\n",
    "  return avg_loss, top1_acc, top3_acc, top5_acc, conf_matrix, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 390250,
     "status": "ok",
     "timestamp": 1745363006065,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "zVmDhLPy196D",
    "outputId": "b15a943a-4e8c-442d-8096-08b9fdd8ec34"
   },
   "outputs": [],
   "source": [
    "_, _, _, _, conf_matrix, _, _, _ = evaluate_with_metrics(model, test_loader, loss_fn, device, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1966,
     "status": "ok",
     "timestamp": 1744736720312,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "o_RRAeEX2osf",
    "outputId": "adf48753-6eb1-40df-a1f9-802f29542371"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "football_action_labels = {\n",
    "    0: \"football shoot\",\n",
    "    1: \"football long pass\",\n",
    "    2: \"football short pass\",\n",
    "    3: \"football through pass\",\n",
    "    4: \"football cross\",\n",
    "    5: \"football dribble\",\n",
    "    6: \"football trap\",\n",
    "    7: \"football throw\",\n",
    "    8: \"football diving\",\n",
    "    9: \"football tackle\",\n",
    "    10: \"football steal\",\n",
    "    11: \"football clearance\",\n",
    "    12: \"football block\",\n",
    "    13: \"football press\",\n",
    "    14: \"football aerial duels\"\n",
    "}\n",
    "\n",
    "action_labels = football_action_labels\n",
    "class_names = [action_labels[i] for i in range(len(action_labels))]\n",
    "\n",
    "conf_matrix_n = conf_matrix.astype('float') / conf_matrix.sum(axis=1, keepdims=True)\n",
    "conf_matrix_n = np.nan_to_num(conf_matrix_n)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix_n, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Normalised confusion matrix\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "#save the plot\n",
    "plt.savefig(\"confusion_matrix_normalized.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1744662696428,
     "user": {
      "displayName": "jamie li",
      "userId": "01375334298802915700"
     },
     "user_tz": -60
    },
    "id": "WmnQTUZB_kQI",
    "outputId": "58e52a4c-0a45-4f12-8305-1be6bcbb609c"
   },
   "outputs": [],
   "source": [
    "plt.savefig(\"confusion_matrix_normalized.png\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPS45p1fOMbZDVSDJI6FKrn",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
